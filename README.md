# Konvens2025 Tutorial - Fusing Vision and Language: A Tutorial on Vision-Language Models for Multimodal Content Analysis

The tutorial slides can be found [here](https://docs.google.com/presentation/d/1ATnXBuYWkKQ7Oba0ItJAnU5ex9jbJpTK/edit?usp=sharing&ouid=105794554671820119952&rtpof=true&sd=true), and all associated notebooks are listed at the bottom of the page.

**Overview**
- [Introduction](#introduction)
- [Presenters](#presenters)
- [Agenda](#agenda)
- [Hands on notebooks](#notebooks)

## Introduction

The increasing availability of multimodal data, including images and videos, has led to a surge of interest in multimodal models that combine visual and textual information. This tutorial will provide an in-depth introduction to the latest advances in multimodal models, with a focus on large vision-language models. Through a combination of theoretical explanations, code demonstrations, and hands-on exercises, participants will learn how to apply these models to a range of image and video analysis tasks, including image captioning, visual concept detection, and image retrieval. By the end of the tutorial, attendees will have a solid understanding of the strengths and limitations of these models, enabling them to implement their own multimodal applications.

## **Presenters**

### **Eric Müller-Budack**

Eric Müller-Budack is leading the Visual Analytics Research Group at TIB – Leibniz Information Centre for Science and Technology. He received his PhD from the Leibniz University Hannover in 2021. His main research interests include automatic multimedia indexing, multimedia and multimodal information retrieval, and deep learning for multimedia analysis and retrieval.

### **Sushil Awale** 

Sushil Awale is a research associate in the Visual Analytics research group at TIB – Leibniz Information Centre for Science and Technology and a third-year PhD student at Leibniz Universität Hannover. His research interests center on multimodal modeling and information retrieval.

## Agenda

This tutorial will take **half a day** (a total of **3 hours plus breaks**) and will be presented **in person**. We propose the following program:

### 1. Welcome Session

- **Welcome**  
- **Background (Research Projects)** 
- **Resources:** Sharing tutorial resources with participants  

### 2. From Language and Vision to Vision-Language Models

- **Overview Natural Language Processing (NLP)**
    - Recurrent Neural Network (RNN)
    - Long Short-Term Memory (LSTM)
    - Attention Mechanism & Transformer
  
- **Overview Computer Vision (CV)**
    - Datasets & Preprocessing
    - Convolutional Neural Network (CNN)
    - Vision Transformer (ViT)

- **"Fusion" of Text and Images**
    - Types of Multimodal Fusion Strategies
    - Language-supervised Learning (CLIP: Contrastive Language-Image Pretraining)

- **Demo Session**
- **Domain Adaptation**

### 3. Generative AI and Video Analysis

- **Generative AI**
    - From Large Language Models (LLMs) to Large Vision-language Models (LVLMs)/Multimodal LLMs (MLLMs)
    - Foundations, use cases, and benchmarks of MLLMs

- **Demo Session**

- **From Images to Videos**

- **TIB Visual Analytics**

### 4. Discussion and Closing Session

## Notebooks

- [Classification and Retrieval using CLIP](https://colab.research.google.com/drive/1tDwBohNyz0mE7xKAAgScNpJrQEU1rmZc)
- [Finetuning CLIP with LoRA](https://colab.research.google.com/drive/1HVf9xsvJsoN4_CHxwZxqaAQynYT2L69F)
- [Introduction to LVLMs](https://colab.research.google.com/drive/1SuUxHVAewvT-rMJr-dz5LnlEFCDxArV6)


